# -*- coding: utf-8 -*-
"""English_Portuguese MT

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y3YCShht3jVzMIW5VZbfin6HldNNoGkx
"""

import pandas as pd
import numpy as np



df = pd.read_csv('/content/drive/MyDrive/Pira패_train .csv')

df.shape

df.head(1)

df1 = pd.read_csv('/content/drive/MyDrive/Pira패_test.csv')
df1.shape

df1.head()

# Load the datasets
df = pd.read_csv('/content/drive/MyDrive/Pira패_train .csv')
df1 = pd.read_csv('/content/drive/MyDrive/Pira패_test.csv')

# Combine them into one DataFrame
data = pd.concat([df, df1], ignore_index=True)

# Check the result
data.shape

data= data.drop(
    columns=[col for col in data.columns if col not in ['question_en_origin', 'question_pt_origin']]
)

data = data.rename(columns={
    'question_en_origin': 'english',
    'question_pt_origin': 'portuguese'
})

data.head()

data.isnull().sum()

data.duplicated().sum()

data.drop_duplicates(inplace=True)

from sklearn.model_selection import train_test_split

#Train-test split
train_data, val_data = train_test_split(data, test_size=0.1, random_state=42)

print("Training size:", len(train_data))
print("Validation size:", len(val_data))

# Save splits
train_data.to_csv("train.csv", index=False)
val_data.to_csv("val.csv", index=False)

from datasets import Dataset
from transformers import MarianMTModel, MarianTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq

# Load model directly
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained("Helsinki-NLP/opus-mt-tc-big-en-pt")
model = AutoModelForSeq2SeqLM.from_pretrained("Helsinki-NLP/opus-mt-tc-big-en-pt")

# Convert dataframe into Hugging Face Dataset
train_dataset = Dataset.from_pandas(train_data)
val_dataset = Dataset.from_pandas(val_data)

def preprocess_function(examples):
    inputs = [str(x) for x in examples["english"]]
    targets = [str(x) for x in examples["portuguese"]]

    # Tokenize inputs
    model_inputs = tokenizer(
        inputs,
        max_length=128,
        truncation=True,
        padding="max_length"
    )

    # Tokenize labels
    labels = tokenizer(
        targets,
        max_length=128,
        truncation=True,
        padding="max_length"
    )

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=train_dataset.column_names)
val_dataset = val_dataset.map(preprocess_function, batched=True, remove_columns=val_dataset.column_names)

# Evaluation (BLEU)

def postprocess_text(preds, labels):
    preds = [pred.strip() for pred in preds]
    labels = [[label.strip()] for label in labels]
    return preds, labels

def compute_metrics(eval_preds):
    preds, labels = eval_preds
    if isinstance(preds, tuple):
        preds = preds[0]

    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)

    bleu = sacrebleu.corpus_bleu(decoded_preds, decoded_labels)
    return {"bleu": bleu.score}

print(train_dataset[0])

training_args = Seq2SeqTrainingArguments(
    output_dir="./results",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    weight_decay=0.01,
    save_total_limit=2,
    num_train_epochs=3,
    predict_with_generate=True,
    logging_dir="./logs",
    remove_unused_columns=False
)

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

# Train the model
trainer.train()

!pip install sacrebleu

import sacrebleu

metrics = trainer.evaluate()
print("Validation BLEU:", metrics["eval_bleu"])

from transformers import pipeline

translation_pipeline = pipeline("translation", model=model, tokenizer=tokenizer)

example = "Machine learning is transforming the world."
result = translation_pipeline(example, max_length=128)
print("English:", example)
print("Portuguese:", result[0]['translation_text'])

import random

# pick random samples from validation set
sample_indices = random.sample(range(len(val_data)), 5)
results = []
for i in sample_indices:
    en_text = val_data.iloc[i]["english"]        # original English column
    pt_ref = val_data.iloc[i]["portuguese"]      # reference Portuguese column

    # translate with the trained model
    inputs = tokenizer(en_text, return_tensors="pt", truncation=True).to(model.device)
    outputs = model.generate(**inputs, max_length=128)
    pt_pred = tokenizer.decode(outputs[0], skip_special_tokens=True)

    results.append({
        "English": en_text,
        "Reference (Portuguese)": pt_ref,
        "Model Output (Portuguese)": pt_pred
    })

df_results = pd.DataFrame(results)
df_results

# Save model + tokenizer
trainer.save_model("./mt_model")
tokenizer.save_pretrained("./mt_model")

from huggingface_hub import login

login()

!pip install huggingface_hub